%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% From a template maintained at https://github.com/jamesrobertlloyd/cbl-tikz-poster
%
% Code near the top should be fairly standard and not need to be changed
%  - except for the document class
% Code lower down is more likely to be customised
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[landscape,a1,final,a4resizeable]{include/a0poster}

%%% Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{multicol}
\setlength{\columnseprule}{0pt}
\def\columnseprulecolor{\color{white}}
% \usepackage[srgb]{xcolor}
\usepackage{morefloats}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{array}
\usepackage{booktabs}
%\usepackage{shortcuts}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[margin=1.5cm]{geometry}
\usepackage[pan gram]{blindtext}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{mwhmacros}
\usepackage{include/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=black, rectangle]
\usepackage{dsfont}
\input{include/jlposter.tex}
% \input{include/preamble.sty}

%%% Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\X}{\B X}

\newcommand{\elbo}{\mathcal{L}}
\newcommand{\entropy}{\mathbb{H}}
\newcommand{\kl}{\mathbb{K}\mathbb{L}}
\newcommand{\eqnn}[1]{\begin{align}{#1}\end{align}}
\newcommand{\eqn}[1]{\begin{align*}{#1}\end{align*}} 
\newcommand{\D}{\mathcal{D}}
\newcommand{\bb}[1]{\ensuremath{ \left( {#1} \right)}}
\newcommand{\BB}[1]{\ensuremath{\left[ {#1} \right]}} 
\newcommand{\half}{\frac{1}{2}}
\newcommand{\B}[1]{\ensuremath{  \mathbf{#1} } }
\newcommand{\pbnn}{p_{\text{\tiny BNN}}}
\newcommand{\pgp}{p_{\text{\tiny GP}}}
\newcommand{\amin}[1]{\underset{#1}{\text{argmin }}}
\newcommand{\myItem}{\item[\color{camLightBlue}$\bullet$]}
\newcommand{\col}[1]{{\color{camLightBlue}\textbf{#1}}}%{\fbox{#1}}
% \newcommand{\myBox}[1]{\fbox{#1}}%{#1}%

%%% Color Definitions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{camBlue}{rgb}{.629,.757,.678}
\definecolor{camLightBlue}{rgb}{.416,.678,.894}
\definecolor{camDarkBlue}{rgb}{.208,.339,.447}
\definecolor{darkgreen}{rgb}{0,0.8,0}
\definecolor{darkblue}{rgb}{0,0.08,0.45}
\definecolor{blue}{rgb}{0,0,1}
%\definecolor{bordercol}{RGB}{40,40,40}
%\definecolor{bordercol}{RGB}{150,150,150}
\definecolor{bordercol}{RGB}{0,0,0}
%\definecolor{headercol1}{RGB}{186,215,230}
\definecolor{headercol1}{RGB}{200,200,200}
%\definecolor{headercol2}{RGB}{80,80,80}
\definecolor{headercol2}{RGB}{255,255,255}
%\definecolor{headerfontcol}{RGB}{0,0,255}
\definecolor{headerfontcol}{RGB}{0,0,0}
%\definecolor{boxcolor}{RGB}{186,215,230}
\definecolor{boxcolor}{RGB}{255,255,255}


%%% Eye Cacther %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{minipage}[t][0pt]{\linewidth}
\hspace{-8cm} 
%Align with edge of page, not margin
\vspace{-1.5cm}
\begin{tikzpicture}[overlay]
    \draw [fill=camLightBlue,draw=none] (0, 5) rectangle (100,-6.9);
    \draw [fill=camLightBlue,draw=none] (0, -56.7) rectangle (100,-100);
\end{tikzpicture}

{
\hspace{-0cm}%
\parbox{.08\textwidth}{\hspace{-2cm} 
  % \vspace{-.5cm}
  \includegraphics[height=8cm]{badges/Utoronto_badge} \hspace{-4cm}%
} 
\parbox{.09\textwidth}{\hspace{-1cm}
  \vspace{-.5cm}\mbox{\includegraphics[height=6cm]{badges/cambridge_badge} \hspace{1cm}}
  }
\hspace{-2cm}
\parbox{.75\textwidth}{
  \vspace{0.5cm}%
  \begin{center}\color{white}
   \Huge \textbf{Mapping Gaussian Processes to Bayesian Neural Networks}\vspace{3mm}\\
    \huge Daniel Flam-Shepherd$^{\,1}$, James Requeima$^{\,2\,3}$, David Duvenaud$^{\,1}$\\
    \color{white}\large 
    $1$ University of Toronto, $2$ University of Cambridge, $3$ Invenia Labs
    \end{center}
  }%
\hspace{1cm}
\parbox{.05\textwidth}{  
  \begin{flushright}\vspace{0.5cm}\includegraphics[height=6cm]{badges/inv_labs}\hfill% 
  \end{flushright}}
}%


%%%  Main Body %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{poster}%
% Potentially add some space at the top of the poster
\vspace{0\baselineskip}%
\vspace*{0.7cm}%
\large%
\noindent%

\begin{multicols}{3}

%%% First Column %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{minipage}[t][47.5cm][t]{.32\textwidth}



\mysection{Priors in Function Space are Interpretable}{

%\vspace{-1cm}
\begin{itemize}
  \item \col{\bf Baysian Neural Network Priors} are specified in parameter space. The implications of these priors in function space are hard to interpret.
  \item How to we incorporate prior knowledge about function properties in our prior?
  \item \col{Gaussian Processes} can elegantly  incorporate prior beliefs about functions through the mean and covariance functions.
\end{itemize}
\vspace{1cm}
\includegraphics[width=\textwidth]{figures/kernels.pdf}
\begin{itemize}
	\item Can we specify BNN priors using GP machinery?
\end{itemize}
}

\vspace{1cm}
\mysection{Minimizing Divergence in Function Space}{

How can we find a prior on weights $p(w)$ that produces functions similar to a GP prior?

$$ \bm \phi ^* =  \amin{\bm \phi}   \E _{\X \sim p(\X)}  \kl [\pbnn (\bm f(\X) |  \bm \phi ) | \pgp (\bm f(\X )  ] $$
\begin{eqnarray*}
\kl [\pbnn (\bm f(\X) |  \bm \phi ) || \pgp (\bm f(\X )  ] = \qquad \qquad \qquad \qquad \\   
-\entropy [\pbnn (\bm f(\X) |  \bm \phi ) ]
-\E_{\pbnn (\bm f |  \bm \phi ) 
} [\log \pgp (\bm f (\X)  ) ]     
\end{eqnarray*}
We can estimate the likelihood with simple Monte Carlo, but how can we estimate the entropy of $p_{BNN}$?
%\begin{align*}
%	\E _{\X \sim p(\X)} \E_{\pbnn (\bm f |  \bm \phi ) } [\log \pgp (\bm f ( \X ) ) ]  \approx  
%  \frac{1}{S} \sum_{s=1}^S \E [\log \pgp (\bm f ^{(s)}(\X) ) ]
%\end{align*}
%where $\bm f ^{(s)}(\X) \sim \pbnn (\bm f | \X )$. What about $\entropy [\pbnn (\bm f (\X) |  \bm \phi ) ] $?
%}

}

\end{minipage}

%%% Second Column %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hspace{0.25cm}
\begin{minipage}[t][47.5cm][t]{.32\textwidth}


%\vspace{1cm}
 
\mysection{Estimating the Entropy of BNN Priors}{

\col{1) Moment matching:}

Approximate {${\pbnn (\bm f (\X) |  \bm \phi ) \approx \N (\bm f | \bm \mu_{\bm f(\X) },\ \bm \Sigma _{\bm f(\X) } )}$}
using empirical mean and covariance. In this case, 
$$\entropy [\pbnn (\bm f (\X) |  \bm \phi ) ] \approx \half \log |2\pi e \bm \Sigma _{\bm f(\X) }| $$
Reasonable since $\pbnn (\bm f |  \bm \phi ) \to \pgp (\bm f  ) $ over $\X \sim p(\X)$.
\vspace{1cm}

\col{2) Nested Variational Bound:}

Introduce noise $\B y = \bm f +\bm \epsilon$, and approximate $\log p (\B y( \X)|  \bm \phi)$ with a variational lower bound $ \elbo{} (\bm \lambda, \B y^{(s)}, \bm X)$.
\begin{eqnarray*}
-\entropy [\pbnn (\B y (\X) |  \bm \phi ) ] &\approx& \frac{1}{S} \sum_{s=1}^S \log \pbnn (\B y ^{(s)}(\X)|  \bm \phi)\\ &\approx& \frac{1}{S} \sum_{s=1}^S \max_{\bm \lambda}  \elbo(\bm \lambda, \B y^{(s)}, \bm X)
\end{eqnarray*}
}
\col{3) Early stopping} to avoid mode collapse. 

\vspace{0.5cm}

\mysection{Samples from Approximate Priors}{
\setlength{\tabcolsep}{-0.25cm}

\begin{center}  
\textcolor{Green}{Gaussian Process Prior}\\
\includegraphics[width=0.8\textwidth]{figures/prior_GP.pdf}\\
%\vspace{-1cm}
%$$\textcolor{Green}{\bm f(\X) \sim \pgp (\bm f | \D )} $$
\textcolor{Blue}{Standard BNN Prior}\\
\includegraphics[width=0.8\textwidth]{figures/prior_BNN.pdf} \\   
%\vspace{-1cm}
\textcolor{Red}{GP-BNN Prior}\\
\includegraphics[width=0.8\textwidth]{figures/prior_GPBNN.pdf} \\
\end{center}


}


\end{minipage}


%%% Thirtd Column %%%
\hspace{0.5cm}
\begin{minipage}[t][14cm][t]{.3\textwidth}

%\vspace{1cm}


\mysection{Posterior Results using GP Priors}{

We test our model (blue) on 2 different toy problems. 
All BNNs are 2 layer with rbf activation functions. The GP priors used RBF kernels. 

%\hspace{2cm} $f(x) =x\sin x/10$ \hspace{4cm}  $f(x)= e^{-x^2/2} $
\begin{center}  
\textcolor{Green}{Gaussian Process Posterior}\\
\includegraphics[width=.48\textwidth]{figures/xsinxgp.pdf}
\includegraphics[width=.48\textwidth]{figures/expxgp.pdf}
%\vspace{-1cm}
%$$\textcolor{Green}{\bm f(\X) \sim \pgp (\bm f | \D )} $$
\textcolor{Blue}{Standard BNN Posterior}\\
\includegraphics[width=.48\textwidth]{figures/xsinxbnn.pdf}    
\includegraphics[width=.48\textwidth]{figures/expxbnn.pdf}       
%\vspace{-1cm}
\textcolor{Red}{GP-BNN Posterior}\\
\includegraphics[width=.48\textwidth]{figures/xsinxgppbnn.pdf} 
\includegraphics[width=.48\textwidth]{figures/expxgppbnn.pdf}    
%\vspace{-1cm}
%$$\textcolor{Blue}{\bm f(\X) \sim \pbnn (\bm f(\bm X)|\varphi^*) }$$
\end{center}
%\col{Results: } 
%The mean and uncertainty bands of the posterior trained with the optimized prior
%closer resembles the uncertainty of the Gaussian process. 


}

\vspace{1cm}

\mysection{Manifesto}
\begin{itemize}
\item Specifying properties of functions should be the first decision, and computational architecture follows.
\item Priors on functions are more interpretable than priors on parameters.
%\item Parameterization should be an implementation detail
\end{itemize}
\end{minipage}

\end{multicols}
\end{poster}
\end{minipage}
\end{document}

